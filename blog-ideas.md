
- from bert to modernbert !!
- transformers, transformers!!
	- why did we remove recurrance?
	- must be easily understnable.
	- code must be available and explained.
	- diagrams for batch, text and token level undedrstanding of flow.
- deep learning semantics. 
	- kind of like deep learning for people who know how the mechanics work.
	- real questions ansewred:
		- why do we update gradient instead of directly change weight?
		- how are we assured of bypassig local minimas?
		- why is softmax everywhere?
		- why log everywhere?
		- what does activation fucntion really do?
	

